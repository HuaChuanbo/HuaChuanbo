---
layout:     post
title:      Information Theory Notes
subtitle:   整理复习信息论的笔记
date:       2019-04-12
author:     HuaChuanbo
header-img: img/bg_img/hacker.jpg
catalog: true
tags:
    - Information Theory
    - Notes
---

Update. 4.02 - 仅上传，未整理。

# Chapter 1 

## 信息的定义*

1. **常识理解**：我们在适应外部世界，控制外部世界的过程中同外部世界交换的内容的名称。

2. **狭义定义**：凡是在一种情况下能减少不确定性的任何事物都叫信息。

   信息是物质存在的一种方式、形态或运动形态，也是事物的一种普遍属性，一般指数据、消息中所包含的意义，可以使消息中所描述事件中的不定性减少。

3. **哲学定义**：信息是一切物质的属性，信息就是信息，不是物质不是能量。



## 信息的特征*

1. 信息必须依赖于某种载体。
2. 信息需要在交换、交流、分析和比较中发挥作用。
3. 信息具有可设计、传递、复制、储存、修改和扩展等特性。
4. 信息可以度量。常见指标有：
   - 正确率
   - 处理速度或速率
   - 处理空间
   - 其他指标：容错率、稳定性与自适应性、技术要求



## C.E.Shannon 信息论的建立与发展*

**1948 - 1960s** ：香农信息论理论确立期。对信息论中各种问题实现严格数学描述与论证。

**1970s - 1980s** ：香农信息论理论发展期。主要集中在“率失真理论”和“多用户信息论”。代数编码理论快速发展，产生各种代数码和概率码。

**1990s** ：理论成功与多学科结合，得到迅速发展。

- 无失真信源编码的应用
- 有失真信源数据压缩编码的应用
- 调至调节码问题
- Turbo 码和 LDPC 码



## 通信模型*

信源 ——> 信源编码 ——> 信道编码 ——> 信道（噪音）——> 信道译码 ——> 信源译码 ——> 信宿



# Chapter 2 

> 信息的度量 = f (复杂度，概率分布)

## 香农熵*

> 随机变量不确定性的度量。

香农熵都是以概率分布或者随机变量为基础进行定义的，使用 $X,Y,Z$ 来表示随机变量，一个**随机变量**和它的**概率分布**记为：

$$X\sim\begin{pmatrix} x_1 & x_2 & ... & x_a \\ p_1 & p_2 & ... & p_a \\ \end{pmatrix}$$

- $\chi=\{x_1,x_2,...,x_a\}$ 是随机变量 $X$ 的**取值空间**。

- $a=||\chi||$ 表示 $\chi$ 所含的**元素个数**。
- $p_i=P_r\{X=x_i\},i=1,2,...,a$ 是 $X$ 取值为 $x_i$ 的**概率**。$P_r\{A\}$ 表示随机事件 $A$ 的概率。

一个随机变量不确定性是它概率分布的一个函数：$$H(X)=H(p)=H(p_1,...,p_a),p\in P$$

其中：$P$ 是 $X$ 的**全体概率分布**。$p$ 如果是均匀分布，$H(p)$ 记为 $g(a)$。



**离散形式的香农熵**：$H(p_1,p_2,...,p_k)=-\sum_{i=1}^{k}p_ilog_cp_i=\sum_{i=1}^{k}p_ilog_c\frac{1}{p_i}=E(log_c\frac{1}{p(x)})$

> 推导：Page. 35.

单位：bit(2), nat(e), tet(3), det(10).

换底公式：$loc_cx=\frac{loc_bx}{loc_bc}$

性质：

1. $H(p_1,...,p_a)$ 是向量 $p=(p_1,...,p_a)$ 的非负连续函数

2. 对任何正整数 $a$ 必有 $g(a)<f(a+1)$

3. 不确定性的可加性：任意一组正整数 $b_i,i=1,2,...,k$ 如果 $\sum_{i=1}^{k}b_i=a$ 那么

   $g(a)=H(b_1',...,b_k')+\sum_{i=1}^{k}b_i'g(b_i)$

4. 对称性：$(p_1,p_2,...,p_a)$ 顺序可以任意变换

5. 非负性：$H(p_1,...,p_a)\geq0$



**联合熵**：$H(X,Y)=-\sum_{x\in\chi}\sum_{y\in Y}P(x,y)log_cp(x,y)$

$X,Y$ 独立情况下：$H(X,Y)=H(X)+H(Y)$

**条件熵**：$H(X|Y)=-\sum_{x\in\chi}\sum_{y\in Y}P(x,y)log_cp(x|y)$

与联合熵的关系：$H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)$

**Markov 链**：$X^n=(X_1,...,X_n)$ 满足 $(x_1,...,x_{i-1})\in \chi^{i-1},x_i,x_{i+1}\in \chi$

$p((x_1,...,x_n),x_{i+1}|x_i)=p(x_1,...,x_{i-1}|x_i)·p(x_{i+1}|x_i)$ 成立

此时：$H(X_1,...,X_n)=\sum_{i=1}^{n}H(X_i|x_{i-1})$



## 熵率*

实际意义：信源平均每个符号所携带的信息量。

X 的不确定程度（决策树）

确定信源所发送信息需要做的“功”

表示信源 X 的信息所需要的字符长度



## 熵的基本性质*

1. 唯一性、极值性：$H(X)\leq log_ca,(1-\frac{1}{x}\leq lnx\leq x-1)$ 
2. 非负性：$H(p_1,...,p_a)\geq0$
3. 可加性：$H(X)=H(P)+\sum_{i=1}^{q}p_i H(q_i')$
4. 对称性：$(p_1,p_2,...,p_a)$ 顺序可以任意变换
5. 上凸性：在 Jenson 不等式中会用到
6. 各种熵之间的关系
7. 几个常用的不等式：
   - Fano 不等式：$X,Y\subset \chi,p_e=P_r \{X\neq Y\}$ 则：$H(X|Y)\leq H(P_e)+P_elog(a-1)$

   - Jenson 不等式：函数 $g(x)$ 在 $(a,b)$ 上凸，则： $E[g(X)]\leq g[E(X)]$

     推广：$g(\sum_{i=1}^{k}\lambda_ix_i)\geq \sum_{i=1}^{k}\lambda_ig(x_i)$

   - $lnx \le x-1 (x \ge 0)$



## 互熵 & 互信息*

> 概率分布“差异性”的度量。

**互熵（也称“散度”）**：$K(p;q)=\sum_{x\in \chi}p(x)log\frac{p(x)}{q(x)}$

互熵的性质：$q$ 固定，$K(p;q)$ 是 $p$ 的下凸函数；$p$ 固定，$K(p;q)$ 是 $q$ 的下凸函数。

**互信息（特殊的互熵）**：$I(X;Y)=\sum_{x}\sum_{y}p(x,y)log\frac{p(x,y)}{p(x)p(y)}$ （$p(x,y)$与 $p(x)p(y)$ 的互熵）

条件互信息：$I(X;Y|Z)=\sum_{(x,y,z)}p(x,y,z)log\frac{p(x,y|z)}{p(x|z)p(y|z)}$

互信息的性质：

1. 非负性：$0\le I(X,Y)\le \{H(X),H(Y)\}$
2. 对称性：$I(X;Y)=I(Y;X)$
3. $I(X;X) = H(X)$
4. $I(X;Y)=I(P,Q)$
5. $I((X_1,X_2);Y)=I(X_1;Y)+I(X_2;Y|X_1)$
6. $I(X;Y|Z)=H(X|Z)+H(Y|Z)-H(X,Y|Z)$
7. $I(X;X|Z)=H(X|Z)$
8. 科尔莫戈罗夫公式：$I(X;(Y,Z))=I(X;Y)+I(X;Z|Y)$
9. $X\rightarrow Y\rightarrow Z$ 马尔科夫链 $I(Y;X)\ge I(Z;X)$

**连续型随机变量**的信息量大部分都是由 $\sum$ 更换为积分即可。



## 最大熵原理

> 寻找某种概率分布，使得熵最大。

对固定的概率密度函数族 $P$ ，若存在一个 $p_0\in P$，使得对任何 $p\in P$:

$-\int _{\chi}p(x)logp_0(x)dx=H_0$ 是一个与 $p$ 无关的常数，则 $p_0$ 被称为**最大熵分布**， $H(p_0)$ 是**最大熵**。

1. 有限区间的最大熵：$p_0(x)=\frac{1}{b-a} $此时 $H_0=log(b-a)$
2. 半开区间$(0,\infty)$的最大熵：$p_0(x)=\frac{1}{\mu}e^{-x/\mu}$ 此时 $H_0=1+log\mu ,(\mu=\int_{0}^{\infty}xp(x)dx)$
3. 全直线的最大熵：$p_0(x)=N(\mu,\sigma^2)$ 此时 $H_0=1/2(1+log(2\pi \sigma^2))$



# Chapter 3

> 通信系统概论。

信源 $\to$ 翻码（也作编码）$\to$ 信道（包含噪音） $\to$ 译码 $\to$ 信宿

消息 $\to$ （编码） $\to$ 入口信号 $\to$ （信道）$\to$ 出口信号 $\to$（译码） $\to$ 消息

**码元**：信号的所有情况（信号$\neq$消息）

**码字**：码元的具体字符，e.g. `a -> 0100` 中的 0,1,0,0 都分别是码字

**信源的概率模型**：$\cal{S}=[\chi,p(x)]$ , $\cal{Y}$ 是信宿字母表

**信道的概率模型**：$\cal{C}=[\cal{U},\cal{V},p(u|v)]$ , 其中：$\cal{U}$ 是输入字母集(字母表)；$\cal{V}$ 是输出字母集；$p(u|v)$ 是转移概率分布

**翻码**（也作编码）：$f:\cal{X}\to \cal{U}$

**译码**：$g:\cal{V}\to \cal{Y}$



**通信系统**：$\varepsilon=\{\cal{S},\cal{C}\}=\{\cal{X},p(x),\cal{U},p(u|v),\cal{V}\}$

**有编码的通信系统**：$\varepsilon=\{\cal{S},\cal{C},(f,g)\}=\{\cal{X},p(x),\cal{U},p(u|v),\cal{V},(f,g),\cal{Y}\}$

以随机变量表示：$(X,U,V,Y)$ 满足：$p(x,u,v,y)=p(x)p(u|x)p(v|u)p(y|v)$，构成马尔可夫链

上述定义可以自由推广到通信系统的序列模型（均以多维向量表示）

- 有记忆信源序列：随着信源序列的增长，其概率分布不能独立相乘
- 无记忆信源序列：随着信源序列的增长，概率分布可以有单字母概率相乘获得



# Chapter 4

> 信源编码问题。

## 概论

**信源编码问题***：

1. **有失真信源编码问题**：允许编码运算有一定的误差产生

2. **无失真信源编码问题**：编码运算能百分百还原数据信息

   - **定（等）长码**：信号长度固定（编码简单；编码利用率低）

     定义：$m,n$ 是两个固定的正整数，$f:\cal{X}^m\to\cal{U}^n$ 是一个等长编码

   - **变长码**：信号长度不固定（提高编码效率）

     定义：$f:\cal{X}^m\to\cal{U}^*,\cal{U}^*=\bigcup_{n=1}^{\infty}\cal{U}^n$  为变长编码



**1 - 1码 & 唯一可译码**：

1. 唯一可译码 $\to$ 1 - 1码
2. 1 - 1定长码 $\to$ 唯一可译码
3. 1 - 1变长码不一定是唯一可译码



**即时码 & 前缀码**：

变长码的记号：$\cal{C} = \cal{U}_0^*=\{C_1,...,C_a\}$ 其中 $C_i=(u_{i1},...,u_{ik_i})$ 是码元 

由编码排列成的字符串：$C=(C_{x_1},...,C_{x_n})$

**即时码**：字符串 $C$ 从左向右读，码元一旦出现就可以确定

**前缀码**：任何一个码元都不是另一个码元的前缀

关系：

1. 前缀码一定是即时码，反之不然（如 0, 01, 011, 0111 这组即时码)
2. 前缀码一定是唯一可译码 & 即时码，反之亦然



## 变长编码问题

> 变长编码问题：求唯一可译变长码(最优变长码)，使得其平均码长最短。

**平均码长**：记 $\cal{l}_f(x)$ 是 $C_{x}$ 的长度，则 $L(\cal{S},f)=\sum_{x\in \cal{X}}p(x)\cal{l}_f(x)$ 为该编码的平均码长

**Kraft 不等式**：记 $r=||\cal{U}||$ 如果 $f$ 是一个即时编码，那么：$\sum_{k=1}^{a}{\frac{1}{r^{\cal{l}_k}}} \leq 1$ （Kraft 不等式）成立。反之，若各码元长度 $\cal{l}_f(x)$ 满足 Kraft 不等式，那么肯定**存在一个**满足条件的即时码。

> 推导：Page. 82

推论：

1. 最优**变长即时码**平均码长**下限**：$H_r(p_1,...p_a)\leq L(\cal{S},f)$ 其中 $H_r(p_1,...,p_a)$是熵的算法。当且仅当 $l_i=-log_rp_i$ 时等号成立。 
2. 最优**变长即时码**平均码长**上限**：$L({S},f)  < H_r(p_1,...p_a)+1$  



**Huffman 码与算术码：**

> Huffman 码是最优的前缀码。
>
> 算术码一定是前缀码。

Huffman 编码方法* Page.89 （必考）

算术码编码方法* Page. 90

算术码平均长度满足：$L({S},f)  < H_r(p_1,...p_a)+2$



## 定长编码问题

$$X\sim\begin{pmatrix} x_1 & x_2 & ... & x_a & ... \\ \frac{1}{2} & \frac{1}{2^2} & ... & \frac{1}{2^a} & ... \\ \end{pmatrix}$$

记 $\cal{X}_0$ 是 $\cal{X}$ 的一个子集，满足：$p(\cal{X}_0)>1-\epsilon$ （其含义为：只考虑概率相合为 $1-\epsilon$ 的 x，余下的视为**编码误差**）

通常称 $\cal{X}_0$ 的元素个数 $v$ 为信源 $\cal{S}$ 的**信号体积**

> 在定长编码中，我们不要求编码与译码是一一对应，但是要求 $e_n(f,g)\to 0$。 



## 度量*

熵率：

意义：平均每个符号携带的信息量



## 码的构造*

1. 树型码
2. 最优码：Huffman 码
3. 实用编码

基本理论： Shannon 第一编码定理（码的存在性）



## 典型序列集

1. 高概率
2. $x\in T_x(N,\epsilon)$
3. $|T_x| \approx 2^{H(x)}$



# Chapter 5

> 信道编码定理。

在通信问题中，牺牲数量可以换取质量。

具有均匀分布的信源序列：$p(x^n)=\frac{1}{M_n}$ 其中 $\cal{X}=\{1,2,...,M_n\},n=1,2,3,...$

信源编码在信道序列中是**可通过的**：$n\to \infty$ 时 $e(f^n,g^n)\to 0$



## 信道容量*

离散：$$C(Q)=Max_{\{p\}}I(X,Y) = Max_{\{p\}} I(P,Q)$$

连续：$$C(Q)=Max_{\{p\}}I(X,Y) = sup\{I(X;Y)\}$$

1. 对称信道：输入呈等概率分布
2. K - T 条件
3. 迭代：$I(P,Q)=max_p max_q I(P;Q)$
4. Gauss 信道：功率限制 P。$1/2 ln(1+p/N)$



## 信道编码*

联合典型序列集：

错误概率：

1. 基本定理：Shannon 第二编码定理
2. 实际应用：线性编码
3. 信源信道联合编码：$H(X) < C$ 时可以找到一种联合编码



# Chpater 5

这一章集中在信息论相关的一些证明，鉴于 MD 书写公式实在不是很方便，所以准备在之后使用 Latex 重新书写之后上传。

















